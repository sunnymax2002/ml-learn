{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Making through Natural Language Processing\n",
    "\n",
    "How do we humans make decisions? Let's take an example to understand. Suppose, you want to enjoy the weekend by going to some place with your family. We put this down into simple sentences to visualize how our brain makes decision.\n",
    "\n",
    "* **Objective**: Enjoy the day with family\n",
    "\n",
    "The important keywords in above objective sentence are *enjoy*, *day* and *family*. Therefore, the first step is to extract the keywords from the objective statement.\n",
    "\n",
    "We use the popular NLTK library to extract such information from the objective statement. Here's the flow as taught by NLTK\n",
    "\n",
    "![](http://www.nltk.org/images/ie-architecture.png)\n",
    "\n",
    "Since we're starting with a single, simple sentence, we can skip the first step of tokenizing into sentences. The script below shows how to tokenize the sentence and add Part of Speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "> Enter the objective statement: \n\n> User provided objective statement: \nWe want to return back to India since we're not enjoying living in scotland\n\n> Tagging Objective Statement into Parts-of-Speech:\nWe want to return back to India since we're not enjoying living in scotland\n> Expanding Contractions...\n[('We', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('return', 'VB'), ('back', 'RB'), ('to', 'TO'), ('India', 'NNP'), ('since', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('enjoying', 'VBG'), ('living', 'VBG'), ('in', 'IN'), ('scotland', 'NN')]\n"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "from typing import List\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# References\n",
    "#   NLTK book: http://www.nltk.org/book/\n",
    "#   NLTK sentiment analysis: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "# https://www.lexalytics.com/lexablog/context-analysis-nlphttps://www.lexalytics.com/lexablog/context-analysis-nlp\n",
    "\n",
    "# Reference: https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.htmlhttps://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html\n",
    "# TODO sym-link doesn't work, need to debug and remove copy of map from here\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    print(\"> Expanding Contractions...\")\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Cleans up input sentence (expand contractions, lemmatize, remove punctuations etc.) and converts into Parts of Speech tokens\n",
    "def TagPartsOfSpeech(obj_statement: str, tokenize=True, pos_tag_help=False) -> List[str]:\n",
    "    print(\"\\n> Tagging Objective Statement into Parts-of-Speech:\")\n",
    "    print(obj_statement)\n",
    "\n",
    "    if(tokenize == True):\n",
    "        # Expand contractions, and Break text/sentence into tokens\n",
    "        tokens = nltk.word_tokenize(expand_contractions(obj_statement))\n",
    "    else:\n",
    "        # Input is already tokenized\n",
    "        tokens = obj_statement\n",
    "\n",
    "    # print(\"\\n> NLTK built-in Named Entry parser:\")\n",
    "    # print(nltk.ne_chunk(obj_statement))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # TODO check if right method, changes 'us/pronoun' to 'u/adjective' and \n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Clean punctuations: comma TODO more string.punctuation as we're dealing only with a sentence\n",
    "    tokens = [word for word in tokens if word != ',']\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # Add Part Of Speech tags to tokens\n",
    "    tagged = nltk.pos_tag(tokens) # Use default to allow subsequent classification possible: , tagset='universal')\n",
    "    print(tagged)\n",
    "\n",
    "    # KEEP-HELP: Meaning of each tag, and traversal through all tags\n",
    "    if(pos_tag_help == True):\n",
    "        for w, t in tagged:\n",
    "            print(w, t, '->', nltk.help.upenn_tagset(t))\n",
    "\n",
    "    return tagged\n",
    "\n",
    "# Step 0: Input objective statement from user\n",
    "print(\"> Enter the objective statement: \")\n",
    "obj_statement = \"We want to return back to India since we're not enjoying living in scotland\" # \"I want to enjoy the day with family\" # TODO str(input())\n",
    "\n",
    "print(\"\\n> User provided objective statement: \")\n",
    "print(obj_statement)\n",
    "\n",
    "# Step 1: Tag Objective statement into parts-of-speech\n",
    "tagged = TagPartsOfSpeech(obj_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to detect entities, which are simply groups of words describing an element of the sentence. For example, in the code below, we extract 'Noun Phrase (NP)', which is determiner-adjective(s)-noun chunk.\n",
    "\n",
    "Depending on what information one wants to extract, a RegExp can be defined to extract chunks from PoS tagged sentence. We break the sentence into NP-VP-NP chunks that we call 'Operation' (OPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (OPER\n    (NP We/PRP)\n    (VP want/VBP to/TO return/VB back/RB to/TO)\n    (NP India/NNP))\n  (OPER\n    (NP since/IN we/PRP)\n    (VP are/VBP not/RB enjoying/VBG living/VBG)\n    (NP in/IN scotland/NN)))\n"
    }
   ],
   "source": [
    "# Finds chunks as specified by 'grammer' in PoS tagged sentence 'word_tagged_sent'\n",
    "def FindRegExpChunks(word_tagged_sent, grammer):\n",
    "    cp = nltk.RegexpParser(grammer, loop=1)\n",
    "\n",
    "    np_chunk = cp.parse(word_tagged_sent)\n",
    "    print(np_chunk)\n",
    "\n",
    "    return np_chunk\n",
    "\n",
    "# Reference: http://www.nltk.org/book/ch07.htmlhttp://www.nltk.org/book/ch07.html\n",
    "\n",
    "def ParseSentChunks(word_tagged_sent):\n",
    "    # Chunking example 1\n",
    "    # grammar_np = r\"\"\"\n",
    "    #     NP:\n",
    "    #         {<DT>?<JJ>*<NN>}    # an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)\n",
    "    #         {<NN>+}             # one or more nouns together\n",
    "    #     \"\"\"\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "    NP:   {<DT|RB.*|JJ.*|IN|CD>*<PRP.*|NN.*>+}                         # Chunk sequences of (DT, JJ ...), (PRP, NN...)\n",
    "    VP:   {<IN>?<VB.*|RB|TO>+<IN>*}                             # Chunk for verb-adverb-verb...\n",
    "    OPER:   {<NP><VP><NP>}                # Chunk NP, VP, NP\n",
    "    \"\"\"\n",
    "\n",
    "    #    P_P:   {<IN><NP>}                    # Chunk prepositions followed by NP\n",
    "    #  V_P: {<VB.*><NP|PP|CLAUSE>+$}      # Chunk verbs and their arguments\n",
    "\n",
    "    # CLAUSE: {<NP><VP>}                # Chunk NP, VP\n",
    "\n",
    "    chunked = FindRegExpChunks(word_tagged_sent, grammar)\n",
    "    return chunked\n",
    "\n",
    "    # Chinking example (exlude part of sentence)\n",
    "    # grammer2 = r\"\"\"\n",
    "    #   NP:\n",
    "    #     {<DT><NN>}  # Chunk DT followed by NN\n",
    "    #     }<NN|IN>+{       # Chink sequences of NN and IN\n",
    "    #     {<.*>+}          # Chunk everything\n",
    "    #   \"\"\"\n",
    "    # chunk2 = FindRegExpChunks(tagged, grammer2)\n",
    "\n",
    "chunked = ParseSentChunks(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test our Chunker using a corpus. Note that any general text won't fare well, since we're targetting 'objective statements'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n> Tagging Objective Statement into Parts-of-Speech:\n['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']\n[('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), ('indulgent', 'NN'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'\", \"''\"), ('s', 'JJ'), ('marriage', 'NN'), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')]\n(S\n  (OPER\n    (NP She/PRP)\n    (VP was/VBD)\n    (NP the/DT youngest/JJS of/IN the/DT two/CD daughters/NNS))\n  (NP of/IN a/DT most/RBS affectionate/JJ indulgent/NN father/NN)\n  ;/:\n  and/CC\n  (VP had/VBD)\n  (NP in/IN consequence/NN)\n  (NP of/IN her/PRP$ sister/NN)\n  '/''\n  (OPER (NP s/JJ marriage/NN) (VP been/VBN) (NP mistress/NN))\n  (NP of/IN his/PRP$ house/NN)\n  (NP from/IN a/DT very/RB early/JJ period/NN)\n  ./.)\n\n> Tagging Objective Statement into Parts-of-Speech:\n['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.']\n[('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN'), ('too', 'RB'), ('long', 'RB'), ('ago', 'RB'), ('for', 'IN'), ('her', 'PRP$'), ('to', 'TO'), ('have', 'VB'), ('more', 'JJR'), ('than', 'IN'), ('an', 'DT'), ('indistinct', 'JJ'), ('remembrance', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('caresses', 'NNS'), (';', ':'), ('and', 'CC'), ('her', 'PRP$'), ('place', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('supplied', 'VBN'), ('by', 'IN'), ('an', 'DT'), ('excellent', 'JJ'), ('woman', 'NN'), ('as', 'IN'), ('governess', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('fallen', 'VBN'), ('little', 'JJ'), ('short', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('mother', 'NN'), ('in', 'IN'), ('affection', 'NN'), ('.', '.')]\n(S\n  (OPER\n    (NP Her/PRP$ mother/NN)\n    (VP had/VBD died/VBN)\n    (NP too/RB long/RB ago/RB for/IN her/PRP$))\n  (VP to/TO have/VB)\n  (NP more/JJR than/IN an/DT indistinct/JJ remembrance/NN)\n  (NP of/IN her/PRP$ caresses/NNS)\n  ;/:\n  and/CC\n  (OPER\n    (NP her/PRP$ place/NN)\n    (VP had/VBD been/VBN supplied/VBN)\n    (NP by/IN an/DT excellent/JJ woman/NN))\n  (NP as/IN governess/NN)\n  who/WP\n  (VP had/VBD fallen/VBN)\n  (NP little/JJ short/JJ of/IN a/DT mother/NN)\n  (NP in/IN affection/NN)\n  ./.)\n"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "#print(gutenberg.fileids()[0])\n",
    "emma = nltk.corpus.gutenberg.sents('austen-emma.txt')[4:6]\n",
    "for sent in emma:\n",
    "    #print(sent)\n",
    "    chunked = ParseSentChunks(TagPartsOfSpeech(sent, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a set of 'objective statements' and test our Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n> Tagging Objective Statement into Parts-of-Speech:\nWe want to return back to India since we're not enjoying living in scotland\n> Expanding Contractions...\n[('We', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('return', 'VB'), ('back', 'RB'), ('to', 'TO'), ('India', 'NNP'), ('since', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('enjoying', 'VBG'), ('living', 'VBG'), ('in', 'IN'), ('scotland', 'NN')]\n(S\n  (OPER\n    (NP We/PRP)\n    (VP want/VBP to/TO return/VB back/RB to/TO)\n    (NP India/NNP))\n  (OPER\n    (NP since/IN we/PRP)\n    (VP are/VBP not/RB enjoying/VBG living/VBG)\n    (NP in/IN scotland/NN)))\n\n> Tagging Objective Statement into Parts-of-Speech:\nI wants to enjoy the day with my family\n> Expanding Contractions...\n[('I', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('enjoy', 'VB'), ('the', 'DT'), ('day', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('family', 'NN')]\n(S\n  (OPER (NP I/PRP) (VP wants/VBZ to/TO enjoy/VB) (NP the/DT day/NN))\n  (NP with/IN my/PRP$ family/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nLet's enjoy the weekends by playing cricket\n> Expanding Contractions...\n[('Let', 'VB'), ('us', 'PRP'), ('enjoy', 'VB'), ('the', 'DT'), ('weekends', 'NNS'), ('by', 'IN'), ('playing', 'VBG'), ('cricket', 'NN')]\n(S\n  (VP Let/VB)\n  (OPER (NP us/PRP) (VP enjoy/VB) (NP the/DT weekends/NNS))\n  (VP by/IN playing/VBG)\n  (NP cricket/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nLet us enjoy the weekend by playing cricket\n> Expanding Contractions...\n[('Let', 'VB'), ('us', 'PRP'), ('enjoy', 'VB'), ('the', 'DT'), ('weekend', 'NN'), ('by', 'IN'), ('playing', 'VBG'), ('cricket', 'NN')]\n(S\n  (VP Let/VB)\n  (OPER (NP us/PRP) (VP enjoy/VB) (NP the/DT weekend/NN))\n  (VP by/IN playing/VBG)\n  (NP cricket/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nI want to learn a lot of subjects\n> Expanding Contractions...\n[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('learn', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('subjects', 'NNS')]\n(S\n  (OPER (NP I/PRP) (VP want/VBP to/TO learn/VB) (NP a/DT lot/NN))\n  (NP of/IN subjects/NNS))\n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def ReadObjSentsAndParseChunks():\n",
    "    f = open('objective_statements.txt', 'r')\n",
    "    chunked_sents = []\n",
    "    for line in f:\n",
    "        chunked = ParseSentChunks(TagPartsOfSpeech(line.strip(), pos_tag_help=False))\n",
    "        chunked_sents.append(chunked)\n",
    "\n",
    "    return chunked_sents\n",
    "\n",
    "chunked_sents = ReadObjSentsAndParseChunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the sentences chunked. Now we traverse through each chunk in every statement and try to make 'sense' of the information in the chunk/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "OPER processing:  OPER\nNP processing:  NP\nwords:  ['We']\npos:  ['PRP']\nNoun synset...\nVP processing:  VP\nwords:  ['want', 'to', 'return', 'back', 'to']\npos:  ['VBP', 'TO', 'VB', 'RB', 'TO']\nVerb synset...\nroot:  want [Synset('desire.v.01')]\nwant Synset('desire.v.01')\nroot:  want [Synset('be.v.01')]\nwant Synset('want.v.02')\nroot:  want [Synset('search.v.01')]\nwant Synset('want.v.03')\nroot:  want [Synset('move.v.02')]\nwant Synset('want.v.04')\nroot:  want [Synset('miss.v.06')]\nwant Synset('want.v.05')\nroot:  return [Synset('travel.v.01')]\nreturn Synset('return.v.01')\nroot:  return [Synset('transfer.v.05')]\nreturn Synset('render.v.07')\nroot:  return [Synset('change.v.02')]\nreturn Synset('revert.v.01')\nroot:  return [Synset('mean.v.03')]\nreturn Synset('hark_back.v.01')\nroot:  return [Synset('move.v.02')]\nreturn Synset('return.v.05')\nroot:  return [Synset('act.v.01')]\nreturn Synset('return.v.06')\nroot:  return [Synset('move.v.02')]\nreturn Synset('return.v.07')\nroot:  return [Synset('express.v.02')]\nreturn Synset('retort.v.01')\nroot:  return [Synset('appear.v.02')]\nreturn Synset('come_back.v.01')\nroot:  return [Synset('transfer.v.05')]\nreturn Synset('refund.v.01')\nroot:  return [Synset('move.v.02')]\nreturn Synset('render.v.05')\nroot:  return [Synset('decide.v.01')]\nreturn Synset('reelect.v.01')\nroot:  return [Synset('move.v.02')]\nreturn Synset('fall.v.21')\nroot:  return [Synset('travel.v.01')]\nreturn Synset('return.v.14')\nroot:  return [Synset('produce.v.02')]\nreturn Synset('render.v.04')\nroot:  return [Synset('move.v.02')]\nreturn Synset('return.v.16')\nroot:  back [Synset('act.v.01')]\nback Synset('back.v.01')\nroot:  back [Synset('travel.v.01')]\nback Synset('back.v.02')\nroot:  back [Synset('support.v.01')]\nback Synset('second.v.01')\nroot:  back [Synset('back.v.04')]\nback Synset('back.v.04')\nroot:  back [Synset('transfer.v.05')]\nback Synset('back.v.05')\nroot:  back [Synset('be.v.03')]\nback Synset('back.v.06')\nroot:  back [Synset('compete.v.01')]\nback Synset('bet_on.v.01')\nroot:  back [Synset('change.v.01')]\nback Synset('back.v.08')\nroot:  back [Synset('confirm.v.01')]\nback Synset('back.v.09')\nroot:  back [Synset('change.v.02')]\nback Synset('back.v.10')\nNP processing:  NP\nwords:  ['India']\npos:  ['NNP']\nNoun synset...\nIndia Synset('india.n.01')\nOPER processing:  OPER\nNP processing:  NP\nwords:  ['since', 'we']\npos:  ['IN', 'PRP']\nNoun synset...\nVP processing:  VP\nwords:  ['are', 'not', 'enjoying', 'living']\npos:  ['VBP', 'RB', 'VBG', 'VBG']\nVerb synset...\nroot:  are [Synset('be.v.01')]\nare Synset('be.v.01')\nroot:  are [Synset('be.v.02')]\nare Synset('be.v.02')\nroot:  are [Synset('be.v.03')]\nare Synset('be.v.03')\nroot:  are [Synset('exist.v.01')]\nare Synset('exist.v.01')\nroot:  are [Synset('be.v.05')]\nare Synset('be.v.05')\nroot:  are [Synset('equal.v.01')]\nare Synset('equal.v.01')\nroot:  are [Synset('constitute.v.01')]\nare Synset('constitute.v.01')\nroot:  are [Synset('be.v.08')]\nare Synset('be.v.08')\nroot:  are [Synset('act.v.01')]\nare Synset('embody.v.02')\nroot:  are [Synset('use.v.03')]\nare Synset('be.v.10')\nroot:  are [Synset('be.v.11')]\nare Synset('be.v.11')\nroot:  are [Synset('be.v.01')]\nare Synset('be.v.12')\nroot:  are [Synset('be.v.01')]\nare Synset('cost.v.01')\nroot:  enjoying [Synset('enjoy.v.01')]\nenjoying Synset('enjoy.v.01')\nroot:  enjoying [Synset('use.v.01')]\nenjoying Synset('enjoy.v.02')\nroot:  enjoying [Synset('like.v.02')]\nenjoying Synset('love.v.02')\nroot:  enjoying [Synset('change.v.02')]\nenjoying Synset('enjoy.v.04')\nroot:  enjoying [Synset('delight.v.02')]\nenjoying Synset('delight.v.02')\nroot:  living [Synset('be.v.03')]\nliving Synset('populate.v.01')\nroot:  living [Synset('live.v.02')]\nliving Synset('live.v.02')\nroot:  living [Synset('survive.v.01')]\nliving Synset('survive.v.01')\nroot:  living [Synset('exist.v.02')]\nliving Synset('exist.v.02')\nroot:  living [Synset('be.v.11')]\nliving Synset('be.v.11')\nroot:  living [Synset('change.v.02')]\nliving Synset('know.v.05')\nroot:  living [Synset('live.v.07')]\nliving Synset('live.v.07')\nNP processing:  NP\nwords:  ['in', 'scotland']\npos:  ['IN', 'NN']\nNoun synset...\nin Synset('inch.n.01')\nin Synset('indium.n.01')\nin Synset('indiana.n.01')\nin Synset('in.s.01')\nin Synset('in.s.02')\nin Synset('in.s.03')\nin Synset('in.r.01')\nscotland Synset('scotland.n.01')\nOPER processing:  OPER\nNP processing:  NP\nwords:  ['I']\npos:  ['PRP']\nNoun synset...\nI Synset('iodine.n.01')\nI Synset('one.n.01')\nI Synset('i.n.03')\nI Synset('one.s.01')\nVP processing:  VP\nwords:  ['wants', 'to', 'enjoy']\npos:  ['VBZ', 'TO', 'VB']\nVerb synset...\nroot:  wants [Synset('desire.v.01')]\nwants Synset('desire.v.01')\nroot:  wants [Synset('be.v.01')]\nwants Synset('want.v.02')\nroot:  wants [Synset('search.v.01')]\nwants Synset('want.v.03')\nroot:  wants [Synset('move.v.02')]\nwants Synset('want.v.04')\nroot:  wants [Synset('miss.v.06')]\nwants Synset('want.v.05')\nroot:  enjoy [Synset('enjoy.v.01')]\nenjoy Synset('enjoy.v.01')\nroot:  enjoy [Synset('use.v.01')]\nenjoy Synset('enjoy.v.02')\nroot:  enjoy [Synset('like.v.02')]\nenjoy Synset('love.v.02')\nroot:  enjoy [Synset('change.v.02')]\nenjoy Synset('enjoy.v.04')\nroot:  enjoy [Synset('delight.v.02')]\nenjoy Synset('delight.v.02')\nNP processing:  NP\nwords:  ['the', 'day']\npos:  ['DT', 'NN']\nNoun synset...\nday Synset('day.n.01')\nday Synset('day.n.02')\nday Synset('day.n.03')\nday Synset('day.n.04')\nday Synset('day.n.05')\nday Synset('day.n.06')\nday Synset('day.n.07')\nday Synset('sidereal_day.n.01')\nday Synset('day.n.09')\nday Synset('day.n.10')\nNon-OPER processing:  NP\nNon-OPER processing:  VP\nOPER processing:  OPER\nNP processing:  NP\nwords:  ['us']\npos:  ['PRP']\nNoun synset...\nus Synset('united_states.n.01')\nus Synset('uracil.n.01')\nus Synset('uranium.n.01')\nus Synset('u.n.03')\nVP processing:  VP\nwords:  ['enjoy']\npos:  ['VB']\nVerb synset...\nroot:  enjoy [Synset('enjoy.v.01')]\nenjoy Synset('enjoy.v.01')\nroot:  enjoy [Synset('use.v.01')]\nenjoy Synset('enjoy.v.02')\nroot:  enjoy [Synset('like.v.02')]\nenjoy Synset('love.v.02')\nroot:  enjoy [Synset('change.v.02')]\nenjoy Synset('enjoy.v.04')\nroot:  enjoy [Synset('delight.v.02')]\nenjoy Synset('delight.v.02')\nNP processing:  NP\nwords:  ['the', 'weekends']\npos:  ['DT', 'NNS']\nNoun synset...\nweekends Synset('weekend.n.01')\nweekends Synset('weekend.v.01')\nNon-OPER processing:  VP\nNon-OPER processing:  NP\nNon-OPER processing:  VP\nOPER processing:  OPER\nNP processing:  NP\nwords:  ['us']\npos:  ['PRP']\nNoun synset...\nus Synset('united_states.n.01')\nus Synset('uracil.n.01')\nus Synset('uranium.n.01')\nus Synset('u.n.03')\nVP processing:  VP\nwords:  ['enjoy']\npos:  ['VB']\nVerb synset...\nroot:  enjoy [Synset('enjoy.v.01')]\nenjoy Synset('enjoy.v.01')\nroot:  enjoy [Synset('use.v.01')]\nenjoy Synset('enjoy.v.02')\nroot:  enjoy [Synset('like.v.02')]\nenjoy Synset('love.v.02')\nroot:  enjoy [Synset('change.v.02')]\nenjoy Synset('enjoy.v.04')\nroot:  enjoy [Synset('delight.v.02')]\nenjoy Synset('delight.v.02')\nNP processing:  NP\nwords:  ['the', 'weekend']\npos:  ['DT', 'NN']\nNoun synset...\nweekend Synset('weekend.n.01')\nweekend Synset('weekend.v.01')\nNon-OPER processing:  VP\nNon-OPER processing:  NP\nOPER processing:  OPER\nNP processing:  NP\nwords:  ['I']\npos:  ['PRP']\nNoun synset...\nI Synset('iodine.n.01')\nI Synset('one.n.01')\nI Synset('i.n.03')\nI Synset('one.s.01')\nVP processing:  VP\nwords:  ['want', 'to', 'learn']\npos:  ['VBP', 'TO', 'VB']\nVerb synset...\nroot:  want [Synset('desire.v.01')]\nwant Synset('desire.v.01')\nroot:  want [Synset('be.v.01')]\nwant Synset('want.v.02')\nroot:  want [Synset('search.v.01')]\nwant Synset('want.v.03')\nroot:  want [Synset('move.v.02')]\nwant Synset('want.v.04')\nroot:  want [Synset('miss.v.06')]\nwant Synset('want.v.05')\nroot:  learn [Synset('learn.v.01')]\nlearn Synset('learn.v.01')\nroot:  learn [Synset('learn.v.02')]\nlearn Synset('learn.v.02')\nroot:  learn [Synset('learn.v.01')]\nlearn Synset('memorize.v.01')\nroot:  learn [Synset('learn.v.04')]\nlearn Synset('learn.v.04')\nroot:  learn [Synset('act.v.01')]\nlearn Synset('teach.v.01')\nroot:  learn [Synset('determine.v.08')]\nlearn Synset('determine.v.08')\nNP processing:  NP\nwords:  ['a', 'lot']\npos:  ['DT', 'NN']\nNoun synset...\na Synset('angstrom.n.01')\na Synset('vitamin_a.n.01')\na Synset('deoxyadenosine_monophosphate.n.01')\na Synset('adenine.n.01')\na Synset('ampere.n.02')\na Synset('a.n.06')\na Synset('a.n.07')\nlot Synset('batch.n.02')\nlot Synset('lot.n.02')\nlot Synset('set.n.05')\nlot Synset('fortune.n.04')\nlot Synset('draw.n.04')\nlot Synset('bunch.n.03')\nlot Synset('lot.n.07')\nlot Synset('lot.v.01')\nlot Synset('distribute.v.01')\nNon-OPER processing:  NP\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def GetPhraseWordList(p):\n",
    "    w_lst = []\n",
    "    pos_lst = []\n",
    "    for node in p:\n",
    "        # Assert that node is a tuple of len=2\n",
    "        assert type(node) == tuple\n",
    "        assert len(node) == 2\n",
    "\n",
    "        # Extract word and part-of-speech\n",
    "        w = node[0]\n",
    "        pos = node[1]\n",
    "        w_lst.append(w)\n",
    "        pos_lst.append(pos)\n",
    "\n",
    "    return w_lst, pos_lst\n",
    "\n",
    "def ProcessNPSubTree(t):\n",
    "    print(\"NP processing: \", t.label())\n",
    "    w, pos = GetPhraseWordList(t)\n",
    "    print('words: ', w)\n",
    "    print('pos: ', pos)\n",
    "\n",
    "    print('Noun synset...')\n",
    "    for n in w:\n",
    "        # Find noun synonyms\n",
    "        for syn in wn.synsets(n):\n",
    "            print(n, syn)\n",
    "\n",
    "def ProcessVPSubTree(t):\n",
    "    print(\"VP processing: \", t.label())\n",
    "    w, pos = GetPhraseWordList(t)\n",
    "    print('words: ', w)\n",
    "    print('pos: ', pos)\n",
    "\n",
    "    print('Verb synset...')\n",
    "    for v in w:\n",
    "        # Find verb synonyms\n",
    "        for syn in wn.synsets(v, 'v'):\n",
    "            print('verb, syn, root: ', v, syn, syn.root_hypernyms())\n",
    "\n",
    "def ProcessOperSubTree(t: nltk.tree.Tree):\n",
    "    print(\"OPER processing: \", t.label())\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            if subt.label() == 'NP':\n",
    "                ProcessNPSubTree(subt)\n",
    "            if subt.label() == 'VP':\n",
    "                ProcessVPSubTree(subt)\n",
    "\n",
    "def ProcessNonOperSubTree(t: nltk.tree.Tree):\n",
    "    # TODO handle nested...\n",
    "    print(\"Non-OPER processing: \", t.label())\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            print('non-oper subt: ', subt.label())\n",
    "            for c in subt:\n",
    "                print('\\tchild: ', c)\n",
    "\n",
    "def traverseNltkTree(t: nltk.tree.Tree):\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            if subt.label() == 'OPER':\n",
    "                ProcessOperSubTree(subt)\n",
    "            else:\n",
    "                ProcessNonOperSubTree(subt)\n",
    "\n",
    "for chunked_sent in chunked_sents:\n",
    "    # print(type(chunked_sent))\n",
    "    traverseNltkTree(chunked_sent)\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bitc8226c58ec484784b2102751f26fc1ba",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}