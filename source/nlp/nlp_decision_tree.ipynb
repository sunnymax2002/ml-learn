{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Making through Natural Language Processing\n",
    "\n",
    "How do we humans make decisions? Let's take an example to understand. Suppose, you want to enjoy the weekend by going to some place with your family. We put this down into simple sentences to visualize how our brain makes decision.\n",
    "\n",
    "* **Objective**: Enjoy the day with family\n",
    "\n",
    "The important keywords in above objective sentence are *enjoy*, *day* and *family*. Therefore, the first step is to extract the keywords from the objective statement.\n",
    "\n",
    "We use the popular NLTK library to extract such information from the objective statement. Here's the flow as taught by NLTK\n",
    "\n",
    "![](http://www.nltk.org/images/ie-architecture.png)\n",
    "\n",
    "Since we're starting with a single, simple sentence, we can skip the first step of tokenizing into sentences. The script below shows how to tokenize the sentence and add Part of Speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "> Enter the objective statement: \n\n> User provided objective statement: \nEnjoy the day with family\n\n> Tagging Objective Statement into Parts-of-Speech:\n[('Enjoy', 'VB'), ('the', 'DT'), ('day', 'NN'), ('with', 'IN'), ('family', 'NN')]\n"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "from typing import List\n",
    "import nltk\n",
    "\n",
    "# References\n",
    "#   NLTK book: http://www.nltk.org/book/\n",
    "#   NLTK sentiment analysis: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "\n",
    "def TagPartsOfSpeech(obj_statement: str) -> List[str]:\n",
    "    print(\"\\n> Tagging Objective Statement into Parts-of-Speech:\")\n",
    "\n",
    "    # Break text/sentence into tokens\n",
    "    tokens = nltk.word_tokenize(obj_statement)\n",
    "    # print(tokens)\n",
    "\n",
    "    # Add Part Of Speech tags to tokens\n",
    "    tagged = nltk.pos_tag(tokens) # Use default to allow subsequent classification possible: , tagset='universal')\n",
    "    print(tagged)\n",
    "\n",
    "    return tagged\n",
    "\n",
    "# Step 0: Input objective statement from user\n",
    "print(\"> Enter the objective statement: \")\n",
    "obj_statement = \"Enjoy the day with family\" # TODO str(input())\n",
    "\n",
    "print(\"\\n> User provided objective statement: \")\n",
    "print(obj_statement)\n",
    "\n",
    "# Step 1: Tag Objective statement into parts-of-speech\n",
    "tagged = TagPartsOfSpeech(obj_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to detect entities, which are simply groups of words describing an element of the sentence. For example, in the code below, we extract 'Noun Phrase (NP)', which is determiner-adjective(s)-noun chunk.\n",
    "\n",
    "Depending on what information one wants to extract, a RegExp can be defined to extract chunks from PoS tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S Enjoy/VB (NP the/DT day/NN) with/IN (NP family/NN))\n(S (NP Enjoy/VB) (NP the/DT) (NP day/NN with/IN family/NN))\n"
    }
   ],
   "source": [
    "# Finds chunks as specified by 'grammer' in PoS tagged sentence 'word_tagged_sent'\n",
    "def FindRegExpChunks(word_tagged_sent, grammer):\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "\n",
    "    np_chunk = cp.parse(word_tagged_sent)\n",
    "    print(np_chunk)\n",
    "\n",
    "    return np_chunk\n",
    "\n",
    "# Reference: http://www.nltk.org/book/ch07.htmlhttp://www.nltk.org/book/ch07.html\n",
    "\n",
    "# Chunking example 1\n",
    "grammer1 = \"NP: {<DT>?<JJ>*<NN>} # This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)\"\n",
    "chunk1 = FindRegExpChunks(tagged, grammer1)\n",
    "\n",
    "# Chinking example (exlude part of sentence)\n",
    "grammer2 = r\"\"\"\n",
    "  NP:\n",
    "    {<DT><NN>}  # Chunk DT followed by NN\n",
    "    }<NN|IN>+{       # Chink sequences of NN and IN\n",
    "    {<.*>+}          # Chunk everything\n",
    "  \"\"\"\n",
    "chunk2 = FindRegExpChunks(tagged, grammer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "    # Break into chunks\n",
    "    entities = DetectEntity(tagged)\n",
    "    print(entities)\n",
    "\n",
    "    # TODO Find synonyms etc.\n",
    "    # for w, t in tagged:\n",
    "    #     for syn in wordnet.synsets(w):\n",
    "    #         print(w, t, syn)\n",
    "\n",
    "    # KEEP-HELP: Meaning of each tag, and traversal through all tags\n",
    "    # for w, t in tagged:\n",
    "        # print(w, t, '->', nltk.help.upenn_tagset(t))\n",
    "\n",
    "    # KEEP-HELP: \n",
    "    # entities = nltk.chunk.ne_chunk(tagged)\n",
    "    # print(entities)\n",
    "    return tagged\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bitc8226c58ec484784b2102751f26fc1ba",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}