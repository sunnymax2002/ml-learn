{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Making through Natural Language Processing\n",
    "\n",
    "How do we humans make decisions? Let's take an example to understand. Suppose, you want to enjoy the weekend by going to some place with your family. We put this down into simple sentences to visualize how our brain makes decision.\n",
    "\n",
    "* **Objective**: Enjoy the day with family\n",
    "\n",
    "The important keywords in above objective sentence are *enjoy*, *day* and *family*. Therefore, the first step is to extract the keywords from the objective statement.\n",
    "\n",
    "We use the popular NLTK library to extract such information from the objective statement. Here's the flow as taught by NLTK\n",
    "\n",
    "![](http://www.nltk.org/images/ie-architecture.png)\n",
    "\n",
    "Since we're starting with a single, simple sentence, we can skip the first step of tokenizing into sentences. The script below shows how to tokenize the sentence and add Part of Speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from typing import List\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# References\n",
    "#   NLTK book: http://www.nltk.org/book/\n",
    "#   NLTK sentiment analysis: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "# https://www.lexalytics.com/lexablog/context-analysis-nlphttps://www.lexalytics.com/lexablog/context-analysis-nlp\n",
    "\n",
    "# Reference: https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.htmlhttps://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html\n",
    "# TODO sym-link doesn't work, need to debug and remove copy of map from here\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    print(\"> Expanding Contractions...\")\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Cleans up input sentence (expand contractions, lemmatize, remove punctuations etc.) and converts into Parts of Speech tokens\n",
    "def TagPartsOfSpeech(obj_statement: str, tokenize=True, pos_tag_help=False) -> List[str]:\n",
    "    print(\"\\n> Tagging Objective Statement into Parts-of-Speech:\")\n",
    "    print(obj_statement)\n",
    "\n",
    "    if(tokenize == True):\n",
    "        # Expand contractions, and Break text/sentence into tokens\n",
    "        tokens = nltk.word_tokenize(expand_contractions(obj_statement))\n",
    "    else:\n",
    "        # Input is already tokenized\n",
    "        tokens = obj_statement\n",
    "\n",
    "    # print(\"\\n> NLTK built-in Named Entry parser:\")\n",
    "    # print(nltk.ne_chunk(obj_statement))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # TODO check if right method, changes 'us/pronoun' to 'u/adjective' and \n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Clean punctuations: comma TODO more string.punctuation as we're dealing only with a sentence\n",
    "    tokens = [word for word in tokens if word != ',']\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # Add Part Of Speech tags to tokens\n",
    "    tagged = nltk.pos_tag(tokens) # Use default to allow subsequent classification possible: , tagset='universal')\n",
    "    print(tagged)\n",
    "\n",
    "    # KEEP-HELP: Meaning of each tag, and traversal through all tags\n",
    "    if(pos_tag_help == True):\n",
    "        for w, t in tagged:\n",
    "            print(w, t, '->', nltk.help.upenn_tagset(t))\n",
    "\n",
    "    return tagged\n",
    "\n",
    "# Step 0: Input objective statement from user\n",
    "print(\"> Enter the objective statement: \")\n",
    "obj_statement = \"We want to return back to India since we're not enjoying living in scotland\" # \"I want to enjoy the day with family\" # TODO str(input())\n",
    "\n",
    "print(\"\\n> User provided objective statement: \")\n",
    "print(obj_statement)\n",
    "\n",
    "# Step 1: Tag Objective statement into parts-of-speech\n",
    "tagged = TagPartsOfSpeech(obj_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to detect entities, which are simply groups of words describing an element of the sentence. For example, in the code below, we extract 'Noun Phrase (NP)', which is determiner-adjective(s)-noun chunk.\n",
    "\n",
    "Depending on what information one wants to extract, a RegExp can be defined to extract chunks from PoS tagged sentence. We break the sentence into NP-VP-NP chunks that we call 'Operation' (OPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finds chunks as specified by 'grammer' in PoS tagged sentence 'word_tagged_sent'\n",
    "def FindRegExpChunks(word_tagged_sent, grammer):\n",
    "    cp = nltk.RegexpParser(grammer, loop=1)\n",
    "\n",
    "    np_chunk = cp.parse(word_tagged_sent)\n",
    "    print(np_chunk)\n",
    "\n",
    "    return np_chunk\n",
    "\n",
    "# Reference: http://www.nltk.org/book/ch07.htmlhttp://www.nltk.org/book/ch07.html\n",
    "\n",
    "def ParseSentChunks(word_tagged_sent):\n",
    "    # Chunking example 1\n",
    "    # grammar_np = r\"\"\"\n",
    "    #     NP:\n",
    "    #         {<DT>?<JJ>*<NN>}    # an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)\n",
    "    #         {<NN>+}             # one or more nouns together\n",
    "    #     \"\"\"\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "    NP:   {<DT|RB.*|JJ.*|IN|CD>*<PRP.*|NN.*>+}                         # Chunk sequences of (DT, JJ ...), (PRP, NN...)\n",
    "    VP:   {<IN>?<VB.*|RB|TO>+<IN>*}                             # Chunk for verb-adverb-verb...\n",
    "    OPER:   {<NP><VP><NP>}                # Chunk NP, VP, NP\n",
    "    \"\"\"\n",
    "\n",
    "    #    P_P:   {<IN><NP>}                    # Chunk prepositions followed by NP\n",
    "    #  V_P: {<VB.*><NP|PP|CLAUSE>+$}      # Chunk verbs and their arguments\n",
    "\n",
    "    # CLAUSE: {<NP><VP>}                # Chunk NP, VP\n",
    "\n",
    "    chunked = FindRegExpChunks(word_tagged_sent, grammar)\n",
    "    return chunked\n",
    "\n",
    "    # Chinking example (exlude part of sentence)\n",
    "    # grammer2 = r\"\"\"\n",
    "    #   NP:\n",
    "    #     {<DT><NN>}  # Chunk DT followed by NN\n",
    "    #     }<NN|IN>+{       # Chink sequences of NN and IN\n",
    "    #     {<.*>+}          # Chunk everything\n",
    "    #   \"\"\"\n",
    "    # chunk2 = FindRegExpChunks(tagged, grammer2)\n",
    "\n",
    "chunked = ParseSentChunks(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test our Chunker using a corpus. Note that any general text won't fare well, since we're targetting 'objective statements'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "#print(gutenberg.fileids()[0])\n",
    "emma = nltk.corpus.gutenberg.sents('austen-emma.txt')[4:6]\n",
    "for sent in emma:\n",
    "    #print(sent)\n",
    "    chunked = ParseSentChunks(TagPartsOfSpeech(sent, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a set of 'objective statements' and test our Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def ReadObjSentsAndParseChunks():\n",
    "    f = open('objective_statements.txt', 'r')\n",
    "    chunked_sents = []\n",
    "    for line in f:\n",
    "        chunked = ParseSentChunks(TagPartsOfSpeech(line.strip(), pos_tag_help=False))\n",
    "        chunked_sents.append(chunked)\n",
    "\n",
    "    return chunked_sents\n",
    "\n",
    "chunked_sents = ReadObjSentsAndParseChunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the sentences chunked. Now we traverse through each chunk in every statement and try to make 'sense' of the information in the chunk/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Reference: word visualization at https://visuwords.com/\n",
    "\n",
    "def GetPhraseWordList(p):\n",
    "    w_lst = []\n",
    "    pos_lst = []\n",
    "    for node in p:\n",
    "        # Assert that node is a tuple of len=2\n",
    "        assert type(node) == tuple\n",
    "        assert len(node) == 2\n",
    "\n",
    "        # Extract word and part-of-speech\n",
    "        w = node[0]\n",
    "        pos = node[1]\n",
    "        w_lst.append(w)\n",
    "        pos_lst.append(pos)\n",
    "\n",
    "    # Additional check to ensure 1:1 correspondence b/w word and pos\n",
    "    assert len(w_lst) == len(pos_lst)\n",
    "\n",
    "    return w_lst, pos_lst\n",
    "\n",
    "def ProcessNPSubTree(t):\n",
    "    print(\"NP processing: \", t.label())\n",
    "    w, pos = GetPhraseWordList(t)\n",
    "    print('words: ', w)\n",
    "    print('pos: ', pos)\n",
    "\n",
    "    print('Noun synset...')\n",
    "    for n in w:\n",
    "        # Find noun synonyms\n",
    "        for syn in wn.synsets(n):\n",
    "            print(n, syn)\n",
    "\n",
    "def ProcessVPSubTree(t):\n",
    "    print(\"VP processing: \", t.label())\n",
    "    w, pos = GetPhraseWordList(t)\n",
    "    print('words: ', w)\n",
    "    print('pos: ', pos)\n",
    "\n",
    "    print('Verb synset...')\n",
    "    for v in w:\n",
    "        # Find verb synonyms\n",
    "        for syn in wn.synsets(v, 'v'):\n",
    "            print('verb, syn, root: ', v, syn, syn.root_hypernyms())\n",
    "\n",
    "def ProcessOperSubTree(t: nltk.tree.Tree):\n",
    "    print(\"OPER processing: \", t.label())\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            if subt.label() == 'NP':\n",
    "                ProcessNPSubTree(subt)\n",
    "            if subt.label() == 'VP':\n",
    "                ProcessVPSubTree(subt)\n",
    "\n",
    "def ProcessNonOperSubTree(t: nltk.tree.Tree):\n",
    "    # TODO handle nested...\n",
    "    print(\"Non-OPER processing: \", t.label())\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            print('non-oper subt: ', subt.label())\n",
    "            for c in subt:\n",
    "                print('\\tchild: ', c)\n",
    "\n",
    "def traverseNltkTree(t: nltk.tree.Tree):\n",
    "    for subt in t:\n",
    "        if type(subt) == nltk.tree.Tree:\n",
    "            if subt.label() == 'OPER':\n",
    "                ProcessOperSubTree(subt)\n",
    "            else:\n",
    "                ProcessNonOperSubTree(subt)\n",
    "\n",
    "for chunked_sent in chunked_sents:\n",
    "    # print(type(chunked_sent))\n",
    "    traverseNltkTree(chunked_sent)\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "IGRAPH U--- 7 6 --\n+ attr: color (v), label (v)\n+ edges:\n0--1 0--2 1--3 1--4 2--5 2--6\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<igraph.drawing.Plot at 0x101fe7c0>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"250pt\" height=\"250pt\" viewBox=\"0 0 250 250\" version=\"1.1\">\n<defs>\n<g>\n<symbol overflow=\"visible\" id=\"glyph0-0\">\n<path style=\"stroke:none;\" d=\"M 1.75 0 L 1.75 -8.75 L 8.75 -8.75 L 8.75 0 Z M 1.96875 -0.21875 L 8.53125 -0.21875 L 8.53125 -8.53125 L 1.96875 -8.53125 Z M 1.96875 -0.21875 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-1\">\n<path style=\"stroke:none;\" d=\"M 2.828125 0 L 0.171875 -10.023438 L 1.53125 -10.023438 L 3.054688 -3.453125 C 3.214844 -2.761719 3.359375 -2.078125 3.480469 -1.402344 C 3.734375 -2.46875 3.882813 -3.085938 3.929688 -3.253906 L 5.835938 -10.023438 L 7.4375 -10.023438 L 8.875 -4.949219 C 9.230469 -3.6875 9.488281 -2.503906 9.652344 -1.402344 C 9.777344 -2.03125 9.945313 -2.757813 10.152344 -3.582031 L 11.722656 -10.023438 L 13.054688 -10.023438 L 10.308594 0 L 9.03125 0 L 6.917969 -7.636719 C 6.738281 -8.269531 6.632813 -8.664063 6.601563 -8.8125 C 6.496094 -8.351563 6.398438 -7.957031 6.308594 -7.636719 L 4.183594 0 Z M 2.828125 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-2\">\n<path style=\"stroke:none;\" d=\"M 1.066406 0 L 1.066406 -10.023438 L 2.425781 -10.023438 L 7.691406 -2.152344 L 7.691406 -10.023438 L 8.960938 -10.023438 L 8.960938 0 L 7.601563 0 L 2.335938 -7.875 L 2.335938 0 Z M 1.066406 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-3\">\n<path style=\"stroke:none;\" d=\"M 3.945313 0 L 0.0625 -10.023438 L 1.496094 -10.023438 L 4.101563 -2.742188 C 4.308594 -2.15625 4.484375 -1.609375 4.628906 -1.101563 C 4.78125 -1.648438 4.960938 -2.195313 5.167969 -2.742188 L 7.875 -10.023438 L 9.226563 -10.023438 L 5.304688 0 Z M 3.945313 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-4\">\n<path style=\"stroke:none;\" d=\"M 0.101563 0 L 2.753906 -3.773438 L 0.300781 -7.257813 L 1.839844 -7.257813 L 2.953125 -5.558594 C 3.160156 -5.230469 3.328125 -4.957031 3.460938 -4.742188 C 3.65625 -5.039063 3.839844 -5.308594 4.011719 -5.542969 L 5.234375 -7.257813 L 6.707031 -7.257813 L 4.195313 -3.84375 L 6.898438 0 L 5.386719 0 L 3.898438 -2.257813 L 3.5 -2.863281 L 1.59375 0 Z M 0.101563 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-5\">\n<path style=\"stroke:none;\" d=\"M 0.867188 2.796875 L 0.730469 1.640625 C 0.996094 1.710938 1.230469 1.746094 1.4375 1.75 C 1.703125 1.746094 1.921875 1.699219 2.089844 1.613281 C 2.25 1.519531 2.386719 1.394531 2.496094 1.230469 C 2.570313 1.105469 2.695313 0.796875 2.871094 0.3125 C 2.894531 0.242188 2.929688 0.144531 2.980469 0.015625 L 0.226563 -7.257813 L 1.550781 -7.257813 L 3.0625 -3.054688 C 3.253906 -2.519531 3.429688 -1.960938 3.589844 -1.375 C 3.726563 -1.9375 3.894531 -2.488281 4.09375 -3.027344 L 5.648438 -7.257813 L 6.875 -7.257813 L 4.117188 0.125 C 3.816406 0.917969 3.585938 1.46875 3.425781 1.769531 C 3.203125 2.175781 2.953125 2.472656 2.671875 2.660156 C 2.390625 2.847656 2.050781 2.941406 1.660156 2.945313 C 1.417969 2.941406 1.15625 2.890625 0.867188 2.796875 Z M 0.867188 2.796875 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-6\">\n<path style=\"stroke:none;\" d=\"M 0.273438 0 L 0.273438 -1 L 4.894531 -6.304688 C 4.367188 -6.273438 3.90625 -6.257813 3.507813 -6.261719 L 0.546875 -6.261719 L 0.546875 -7.257813 L 6.480469 -7.257813 L 6.480469 -6.445313 L 2.550781 -1.839844 L 1.789063 -1 C 2.335938 -1.035156 2.855469 -1.054688 3.34375 -1.058594 L 6.699219 -1.058594 L 6.699219 0 Z M 0.273438 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-7\">\n<path style=\"stroke:none;\" d=\"M 0.921875 2.78125 L 0.921875 -7.257813 L 2.042969 -7.257813 L 2.042969 -6.316406 C 2.304688 -6.683594 2.601563 -6.957031 2.9375 -7.144531 C 3.265625 -7.324219 3.671875 -7.417969 4.148438 -7.421875 C 4.765625 -7.417969 5.3125 -7.257813 5.789063 -6.945313 C 6.261719 -6.625 6.617188 -6.175781 6.863281 -5.59375 C 7.101563 -5.011719 7.222656 -4.375 7.226563 -3.683594 C 7.222656 -2.9375 7.089844 -2.269531 6.824219 -1.675781 C 6.554688 -1.082031 6.167969 -0.625 5.664063 -0.308594 C 5.152344 0.0078125 4.617188 0.164063 4.0625 0.164063 C 3.648438 0.164063 3.28125 0.078125 2.957031 -0.09375 C 2.628906 -0.265625 2.359375 -0.484375 2.152344 -0.75 L 2.152344 2.78125 Z M 2.039063 -3.589844 C 2.039063 -2.652344 2.226563 -1.960938 2.605469 -1.515625 C 2.980469 -1.066406 3.4375 -0.84375 3.976563 -0.847656 C 4.519531 -0.84375 4.988281 -1.074219 5.382813 -1.539063 C 5.769531 -2 5.964844 -2.71875 5.96875 -3.691406 C 5.964844 -4.613281 5.773438 -5.304688 5.398438 -5.769531 C 5.015625 -6.226563 4.5625 -6.457031 4.03125 -6.460938 C 3.507813 -6.457031 3.042969 -6.210938 2.640625 -5.726563 C 2.238281 -5.234375 2.039063 -4.523438 2.039063 -3.589844 Z M 2.039063 -3.589844 \"/>\n</symbol>\n</g>\n</defs>\n<g id=\"surface56\">\n<rect x=\"0\" y=\"0\" width=\"250\" height=\"250\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 56 86.667969 L 56 163.332031 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 56 86.667969 L 148 10 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 56 163.332031 L 10 240 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 56 163.332031 L 102 240 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 148 10 L 148 86.667969 \"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 148 10 L 240 86.667969 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(0%,0%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 66 86.667969 C 66 92.191406 61.523438 96.667969 56 96.667969 C 50.476563 96.667969 46 92.191406 46 86.667969 C 46 81.144531 50.476563 76.667969 56 76.667969 C 61.523438 76.667969 66 81.144531 66 86.667969 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(100%,75.294118%,79.607843%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 66 163.332031 C 66 168.855469 61.523438 173.332031 56 173.332031 C 50.476563 173.332031 46 168.855469 46 163.332031 C 46 157.808594 50.476563 153.332031 56 153.332031 C 61.523438 153.332031 66 157.808594 66 163.332031 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(74.509804%,74.509804%,74.509804%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 158 10 C 158 15.523438 153.523438 20 148 20 C 142.476563 20 138 15.523438 138 10 C 138 4.476563 142.476563 0 148 0 C 153.523438 0 158 4.476563 158 10 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 240 C 20 245.523438 15.523438 250 10 250 C 4.476563 250 0 245.523438 0 240 C 0 234.476563 4.476563 230 10 230 C 15.523438 230 20 234.476563 20 240 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(0%,0%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 112 240 C 112 245.523438 107.523438 250 102 250 C 96.476563 250 92 245.523438 92 240 C 92 234.476563 96.476563 230 102 230 C 107.523438 230 112 234.476563 112 240 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 158 86.667969 C 158 92.191406 153.523438 96.667969 148 96.667969 C 142.476563 96.667969 138 92.191406 138 86.667969 C 138 81.144531 142.476563 76.667969 148 76.667969 C 153.523438 76.667969 158 81.144531 158 86.667969 \"/>\n<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 250 86.667969 C 250 92.191406 245.523438 96.667969 240 96.667969 C 234.476563 96.667969 230 92.191406 230 86.667969 C 230 81.144531 234.476563 76.667969 240 76.667969 C 245.523438 76.667969 250 81.144531 250 86.667969 \"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-1\" x=\"49.386719\" y=\"93.175781\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-2\" x=\"50.984375\" y=\"169.84375\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-3\" x=\"143.355469\" y=\"16.511719\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-4\" x=\"6.5\" y=\"245.128906\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-5\" x=\"98.449219\" y=\"245.128906\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-6\" x=\"144.515625\" y=\"91.796875\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-7\" x=\"235.925781\" y=\"91.878906\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "execution_count": 11
    }
   ],
   "source": [
    "# Reference: https://igraph.org/python/doc/tutorial/tutorial.html\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "# Create graph\n",
    "g = ig.Graph.Tree(7, 2)\n",
    "# g.add_vertices(3)\n",
    "# g.add_edges([(0,1), (1,2)])\n",
    "# g = Graph([(0,1), (0,2), (2,3), (3,4), (4,2), (2,5), (5,0), (6,3), (5,6)])\n",
    "\n",
    "# Plot graph/tree\n",
    "g.vs[\"label\"] = [\"W\", \"N\", \"V\", 'x', 'y', 'z', 'p']\n",
    "color_dict = {\"W\": 'blue', \"N\": 'pink', \"V\": 'grey', 'x': 'yellow', 'y': 'blue', 'z': 'red', 'p': 'white'}\n",
    "g.vs[\"color\"] = [color_dict[label] for label in g.vs[\"label\"]]\n",
    "print(g)\n",
    "\n",
    "layout = g.layout(\"tree\")\n",
    "ig.plot(g, layout=layout, bbox=(250,250), margin=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bitc8226c58ec484784b2102751f26fc1ba",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}