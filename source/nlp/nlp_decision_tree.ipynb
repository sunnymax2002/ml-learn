{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Making through Natural Language Processing\n",
    "\n",
    "How do we humans make decisions? Let's take an example to understand. Suppose, you want to enjoy the weekend by going to some place with your family. We put this down into simple sentences to visualize how our brain makes decision.\n",
    "\n",
    "* **Objective**: Enjoy the day with family\n",
    "\n",
    "The important keywords in above objective sentence are *enjoy*, *day* and *family*. Therefore, the first step is to extract the keywords from the objective statement.\n",
    "\n",
    "We use the popular NLTK library to extract such information from the objective statement. Here's the flow as taught by NLTK\n",
    "\n",
    "![](http://www.nltk.org/images/ie-architecture.png)\n",
    "\n",
    "Since we're starting with a single, simple sentence, we can skip the first step of tokenizing into sentences. The script below shows how to tokenize the sentence and add Part of Speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "> Enter the objective statement: \n\n> User provided objective statement: \nWe want to return back to India since we're not enjoying living in scotland\n\n> Tagging Objective Statement into Parts-of-Speech:\nWe want to return back to India since we're not enjoying living in scotland\n> Expanding Contractions...\n[('We', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('return', 'VB'), ('back', 'RB'), ('to', 'TO'), ('India', 'NNP'), ('since', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('enjoying', 'VBG'), ('living', 'VBG'), ('in', 'IN'), ('scotland', 'NN')]\n"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "from typing import List\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# References\n",
    "#   NLTK book: http://www.nltk.org/book/\n",
    "#   NLTK sentiment analysis: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "# https://www.lexalytics.com/lexablog/context-analysis-nlphttps://www.lexalytics.com/lexablog/context-analysis-nlp\n",
    "\n",
    "# Reference: https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.htmlhttps://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html\n",
    "# TODO sym-link doesn't work, need to debug and remove copy of map from here\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    print(\"> Expanding Contractions...\")\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Cleans up input sentence (expand contractions, lemmatize, remove punctuations etc.) and converts into Parts of Speech tokens\n",
    "def TagPartsOfSpeech(obj_statement: str, tokenize=True, pos_tag_help=False) -> List[str]:\n",
    "    print(\"\\n> Tagging Objective Statement into Parts-of-Speech:\")\n",
    "    print(obj_statement)\n",
    "\n",
    "    if(tokenize == True):\n",
    "        # Expand contractions, and Break text/sentence into tokens\n",
    "        tokens = nltk.word_tokenize(expand_contractions(obj_statement))\n",
    "    else:\n",
    "        # Input is already tokenized\n",
    "        tokens = obj_statement\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # TODO check if right method, changes 'us/pronoun' to 'u/adjective' and \n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Clean punctuations: comma TODO more string.punctuation as we're dealing only with a sentence\n",
    "    tokens = [word for word in tokens if word != ',']\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # Add Part Of Speech tags to tokens\n",
    "    tagged = nltk.pos_tag(tokens) # Use default to allow subsequent classification possible: , tagset='universal')\n",
    "    print(tagged)\n",
    "\n",
    "    # KEEP-HELP: Meaning of each tag, and traversal through all tags\n",
    "    if(pos_tag_help == True):\n",
    "        for w, t in tagged:\n",
    "            print(w, t, '->', nltk.help.upenn_tagset(t))\n",
    "\n",
    "    return tagged\n",
    "\n",
    "# Step 0: Input objective statement from user\n",
    "print(\"> Enter the objective statement: \")\n",
    "obj_statement = \"We want to return back to India since we're not enjoying living in scotland\" # \"I want to enjoy the day with family\" # TODO str(input())\n",
    "\n",
    "print(\"\\n> User provided objective statement: \")\n",
    "print(obj_statement)\n",
    "\n",
    "# Step 1: Tag Objective statement into parts-of-speech\n",
    "tagged = TagPartsOfSpeech(obj_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to detect entities, which are simply groups of words describing an element of the sentence. For example, in the code below, we extract 'Noun Phrase (NP)', which is determiner-adjective(s)-noun chunk.\n",
    "\n",
    "Depending on what information one wants to extract, a RegExp can be defined to extract chunks from PoS tagged sentence. We break the sentence into NP-VP-NP chunks that we call CLAUSE (CL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (CL\n    (NP We/PRP)\n    (VP want/VBP to/TO return/VB back/RB to/TO)\n    (NP India/NNP))\n  (CL\n    (NP since/IN we/PRP)\n    (VP are/VBP not/RB enjoying/VBG living/VBG)\n    (NP in/IN scotland/NN)))\n"
    }
   ],
   "source": [
    "# Finds chunks as specified by 'grammer' in PoS tagged sentence 'word_tagged_sent'\n",
    "def FindRegExpChunks(word_tagged_sent, grammer):\n",
    "    cp = nltk.RegexpParser(grammer, loop=1)\n",
    "\n",
    "    np_chunk = cp.parse(word_tagged_sent)\n",
    "    print(np_chunk)\n",
    "\n",
    "    return np_chunk\n",
    "\n",
    "# Reference: http://www.nltk.org/book/ch07.htmlhttp://www.nltk.org/book/ch07.html\n",
    "\n",
    "def ParseSentChunks(word_tagged_sent):\n",
    "    # Chunking example 1\n",
    "    # grammar_np = r\"\"\"\n",
    "    #     NP:\n",
    "    #         {<DT>?<JJ>*<NN>}    # an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)\n",
    "    #         {<NN>+}             # one or more nouns together\n",
    "    #     \"\"\"\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "    NP:   {<DT|RB*|JJ*|IN|CD>*<PRP|NN.*>+}                         # Chunk sequences of (DT, JJ ...), (PRP, NN...)\n",
    "    VP:   {<IN>?<VB.*|RB|TO>+<IN>*}                             # Chunk for verb-adverb-verb...\n",
    "    CL:   {<NP><VP><NP>}                # Chunk NP, VP, NP\n",
    "    \"\"\"\n",
    "\n",
    "    #    P_P:   {<IN><NP>}                    # Chunk prepositions followed by NP\n",
    "    #  V_P: {<VB.*><NP|PP|CLAUSE>+$}      # Chunk verbs and their arguments\n",
    "\n",
    "    # CLAUSE: {<NP><VP>}                # Chunk NP, VP\n",
    "\n",
    "    chunked = FindRegExpChunks(word_tagged_sent, grammar)\n",
    "\n",
    "    # Chinking example (exlude part of sentence)\n",
    "    # grammer2 = r\"\"\"\n",
    "    #   NP:\n",
    "    #     {<DT><NN>}  # Chunk DT followed by NN\n",
    "    #     }<NN|IN>+{       # Chink sequences of NN and IN\n",
    "    #     {<.*>+}          # Chunk everything\n",
    "    #   \"\"\"\n",
    "    # chunk2 = FindRegExpChunks(tagged, grammer2)\n",
    "\n",
    "chunked = ParseSentChunks(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test our Chunker using a corpus. Note that any general text won't fare well, since we're targetting 'objective statements'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n> Tagging Objective Statement into Parts-of-Speech:\n['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']\n[('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), ('indulgent', 'NN'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'\", \"''\"), ('s', 'JJ'), ('marriage', 'NN'), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')]\n(S\n  (NP She/PRP)\n  (VP was/VBD)\n  the/DT\n  youngest/JJS\n  (NP of/IN the/DT two/CD daughters/NNS)\n  of/IN\n  a/DT\n  most/RBS\n  (NP affectionate/JJ indulgent/NN father/NN)\n  ;/:\n  and/CC\n  (VP had/VBD)\n  (NP in/IN consequence/NN)\n  of/IN\n  her/PRP$\n  (NP sister/NN)\n  '/''\n  (CL (NP s/JJ marriage/NN) (VP been/VBN) (NP mistress/NN))\n  of/IN\n  his/PRP$\n  (NP house/NN)\n  (NP from/IN a/DT very/RB early/JJ period/NN)\n  ./.)\n\n> Tagging Objective Statement into Parts-of-Speech:\n['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.']\n[('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN'), ('too', 'RB'), ('long', 'RB'), ('ago', 'RB'), ('for', 'IN'), ('her', 'PRP$'), ('to', 'TO'), ('have', 'VB'), ('more', 'JJR'), ('than', 'IN'), ('an', 'DT'), ('indistinct', 'JJ'), ('remembrance', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('caresses', 'NNS'), (';', ':'), ('and', 'CC'), ('her', 'PRP$'), ('place', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('supplied', 'VBN'), ('by', 'IN'), ('an', 'DT'), ('excellent', 'JJ'), ('woman', 'NN'), ('as', 'IN'), ('governess', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('fallen', 'VBN'), ('little', 'JJ'), ('short', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('mother', 'NN'), ('in', 'IN'), ('affection', 'NN'), ('.', '.')]\n(S\n  Her/PRP$\n  (NP mother/NN)\n  (VP had/VBD died/VBN too/RB long/RB ago/RB for/IN)\n  her/PRP$\n  (VP to/TO have/VB)\n  more/JJR\n  (NP than/IN an/DT indistinct/JJ remembrance/NN)\n  of/IN\n  her/PRP$\n  (NP caresses/NNS)\n  ;/:\n  and/CC\n  her/PRP$\n  (CL\n    (NP place/NN)\n    (VP had/VBD been/VBN supplied/VBN)\n    (NP by/IN an/DT excellent/JJ woman/NN))\n  (NP as/IN governess/NN)\n  who/WP\n  (VP had/VBD fallen/VBN)\n  (NP little/JJ short/JJ of/IN a/DT mother/NN)\n  (NP in/IN affection/NN)\n  ./.)\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "    # TODO Find synonyms etc.\n",
    "    # for w, t in tagged:\n",
    "    #     for syn in wordnet.synsets(w):\n",
    "    #         print(w, t, syn)\n",
    "\n",
    "#print(gutenberg.fileids()[0])\n",
    "emma = nltk.corpus.gutenberg.sents('austen-emma.txt')[4:6]\n",
    "for sent in emma:\n",
    "    #print(sent)\n",
    "    ParseSentChunks(TagPartsOfSpeech(sent, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a set of 'objective statements' and test our Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n> Tagging Objective Statement into Parts-of-Speech:\nWe want to return back to India since we're not enjoying living in scotland\n> Expanding Contractions...\n[('We', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('return', 'VB'), ('back', 'RB'), ('to', 'TO'), ('India', 'NNP'), ('since', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('enjoying', 'VBG'), ('living', 'VBG'), ('in', 'IN'), ('scotland', 'NN')]\n(S\n  (CL\n    (NP We/PRP)\n    (VP want/VBP to/TO return/VB back/RB to/TO)\n    (NP India/NNP))\n  (CL\n    (NP since/IN we/PRP)\n    (VP are/VBP not/RB enjoying/VBG living/VBG)\n    (NP in/IN scotland/NN)))\n\n> Tagging Objective Statement into Parts-of-Speech:\nI wants to enjoy the day with my family\n> Expanding Contractions...\n[('I', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('enjoy', 'VB'), ('the', 'DT'), ('day', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('family', 'NN')]\n(S\n  (CL (NP I/PRP) (VP wants/VBZ to/TO enjoy/VB) (NP the/DT day/NN))\n  with/IN\n  my/PRP$\n  (NP family/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nLet's enjoy the weekends by playing cricket\n> Expanding Contractions...\n[('Let', 'VB'), ('us', 'PRP'), ('enjoy', 'VB'), ('the', 'DT'), ('weekends', 'NNS'), ('by', 'IN'), ('playing', 'VBG'), ('cricket', 'NN')]\n(S\n  (VP Let/VB)\n  (CL (NP us/PRP) (VP enjoy/VB) (NP the/DT weekends/NNS))\n  (VP by/IN playing/VBG)\n  (NP cricket/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nLet us enjoy the weekend by playing cricket\n> Expanding Contractions...\n[('Let', 'VB'), ('us', 'PRP'), ('enjoy', 'VB'), ('the', 'DT'), ('weekend', 'NN'), ('by', 'IN'), ('playing', 'VBG'), ('cricket', 'NN')]\n(S\n  (VP Let/VB)\n  (CL (NP us/PRP) (VP enjoy/VB) (NP the/DT weekend/NN))\n  (VP by/IN playing/VBG)\n  (NP cricket/NN))\n\n> Tagging Objective Statement into Parts-of-Speech:\nI want to learn a lot of subjects\n> Expanding Contractions...\n[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('learn', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('subjects', 'NNS')]\n(S\n  (CL (NP I/PRP) (VP want/VBP to/TO learn/VB) (NP a/DT lot/NN))\n  (NP of/IN subjects/NNS))\n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "f = open('objective_statements.txt', 'r')\n",
    "for line in f:\n",
    "    ParseSentChunks(TagPartsOfSpeech(line.strip(), pos_tag_help=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bitc8226c58ec484784b2102751f26fc1ba",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}